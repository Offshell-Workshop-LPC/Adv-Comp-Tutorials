{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Automatic differentiation, and introduction to CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation\n",
    "\n",
    "Having knowledge of the gradients of a multivariable function with respect to its arguments is a powerful tool in mathematical computations, e.g., in\n",
    "- computing Hessians for optimization problems or extracting uncertainties in a likelihood:\n",
    "$$\\begin{bmatrix} \\frac{\\delta^2 f}{\\delta x_1^2} & \\frac{\\delta^2 f}{\\delta x_1 \\delta x_2} \\\\ \\frac{\\delta^2 f}{\\delta x_1 \\delta x_2} & \\frac{\\delta^2 f}{\\delta x_2^2} \\end{bmatrix}$$\n",
    "- computing gradients to find descent to minimum loss in machine learning:\n",
    "$$\\vec{x}_{n+1} = \\vec{x}_n - \\gamma \\nabla L(\\vec{x})|_{\\vec{x}=\\vec{x}_n}$$\n",
    "\n",
    "\n",
    "There are several ways to compute gradients:\n",
    "- Use symbolic language that maps function names to their definitions and other functions for derivatives. Here is an example using Mathematica:\n",
    "![Derivatives in Mathematica](mathematica_screnshot_diff.png)\n",
    "- Use numerical differentiation with small step sizes.\n",
    "- Embed differentials into variable definitions in programming (**automatic differentiation** - topic of today!).\n",
    "\n",
    "Suppose we have a function $f(g(h(x)))$. The chain rule gives us\n",
    "$$v=h(x)$$\n",
    "$$u=g(v)$$\n",
    "$$y=f(u)$$\n",
    "$$\\frac{\\delta f}{\\delta x} = \\frac{\\delta y}{\\delta u} \\frac{\\delta u}{\\delta v} \\frac{\\delta v}{\\delta x}$$\n",
    "\n",
    "This gives us two directions from which we can start differentiation:\n",
    "- Forward: $\\frac{\\delta y}{\\delta u} \\frac{\\delta u}{\\delta v} \\frac{\\delta v}{\\delta x} \\leftarrow$ start computing derivatives in this direction.\n",
    "- Reverse: start computing derivatives in this direction $\\rightarrow \\frac{\\delta y}{\\delta u} \\frac{\\delta u}{\\delta v} \\frac{\\delta v}{\\delta x}$.\n",
    "\n",
    "Why does it matter? Don't we have the same number of derivatives?\n",
    "\n",
    "$\\rightarrow$ That is correct, but for a function $f: U^n \\to V^m$, we would have a different number of matrix multiplication steps (derivatives $\\equiv$ Jacobian matrices). For the sake of example, assume the dimensionality $k$ for both $g$ and $h$. Then,\n",
    "- forward differentiation involves $(n \\times k^2) + (n \\times k \\times m)$ multiplications, whereas\n",
    "- reverse differentiation would need $(m \\times k^2) + (n \\times k \\times m)$ multiplications.\n",
    "\n",
    "If $n \\cong m$, or $k$ is not large compared to $n$ or $m$, the direction does not matter much. On the other hand, if $k$ is large, it is better to use forward differentiation for $n \\ll m$, and reverse differentiation otherwise.\n",
    "\n",
    "In a typical neural network, gradient descent uses *forward propagation* to calculate the cost for a given loss function , and *backpropagation* (through reverse autodifferentiation - **why?**) to calculate gradients of the cost with respect to the weights at each layer.\n",
    "\n",
    "### Exercise: Dual numbers\n",
    "Consider an extension of a real variable $r \\to (r, \\delta r)$. Denoting $u = (x, \\delta x)$ and $v = (y, \\delta y)$, define the minimal set of algebraic properties as follows:\n",
    "- $-u = (-x, -\\delta x)$\n",
    "- $u^{-1} = (1/x, -\\delta x/x^2)$\n",
    "- $u+v = (x+y, \\delta x + \\delta y)$\n",
    "- $u \\times v = (x \\times y, y \\times \\delta x + x \\times \\delta y)$\n",
    "\n",
    "For $f(x, y) = x^{2 y^3}$, write a simple Python script to compute $f(2,3)$ and $\\frac{\\delta f}{\\delta y} |_{(x,y)=(2,3)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18014398509481984 6.742779949618588e+17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DualValue:\n",
    "  def __init__(self, value, differential):\n",
    "    self.value = value\n",
    "    self.differential = differential\n",
    "  def __add__(self, other):\n",
    "    return DualValue(self.value + other.value, self.differential + other.differential)\n",
    "  def __radd__(self, other):\n",
    "    return DualValue(self.value + other, self.differential)\n",
    "  def __sub__(self, other):\n",
    "    return DualValue(self.value - other.value, self.differential - other.differential)\n",
    "  def __rsub__(self, other):\n",
    "    return DualValue(other - self.value, -self.differential)\n",
    "  def __mul__(self, other):\n",
    "    return DualValue(self.value * other.value, self.differential * other.value + self.value * other.differential)\n",
    "  def __rmul__(self, other):\n",
    "    return DualValue(self.value * other, self.differential * other)\n",
    "  def __truediv__(self, other):\n",
    "    return DualValue(self.value / other.value, (self.differential * other.value - self.value * other.differential) / (other.value * other.value))\n",
    "  def __rtruediv__(self, other):\n",
    "    return DualValue(other / self.value, -self.differential * other / (self.value * self.value))\n",
    "  def __neg__(self):\n",
    "    return DualValue(-self.value, -self.differential)\n",
    "  def __lt__(self, other):\n",
    "    return self.value < other.value\n",
    "  def __le__(self, other):\n",
    "    return self.value <= other.value\n",
    "  def __eq__(self, other):\n",
    "    return self.value == other.value\n",
    "  def __ne__(self, other):\n",
    "    return self.value != other.value\n",
    "  def __gt__(self, other):\n",
    "    return self.value > other.value\n",
    "  def __ge__(self, other):\n",
    "    return self.value >= other.value\n",
    "  def __abs__(self):\n",
    "    return DualValue(abs(self.value), self.differential * np.sign(self.value))\n",
    "  def __str__(self):\n",
    "    return str(self.value)\n",
    "  def __pow__(self, other):\n",
    "    if isinstance(other, DualValue):\n",
    "      return DualValue(self.value**other.value, self.value**other.value * other.differential * np.log(self.value) + self.differential * other.value * self.value**(other.value - 1))\n",
    "    else:\n",
    "      return DualValue(self.value**other, other * self.value**(other - 1) * self.differential)\n",
    "  def __rpow__(self, other):\n",
    "    return DualValue(other**self.value, other**self.value * self.differential * np.log(other))\n",
    "  \n",
    "x = DualValue(2, 0)\n",
    "y = DualValue(3, 1)\n",
    "\n",
    "f = x**(2*y**3)\n",
    "print(f.value, f.differential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorflow for automatic differentiation\n",
    "\n",
    "The main TensorFlow object that allows tracking gradients with respect to some input operations is `GradientTape`. Here is an example way to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F(2,3) = 1.8014398509481984e+16\n",
      "df/dx | (x,y)=(2,3) = 4.863887597560136e+17\n",
      "df/dy | (x,y)=(2,3) = 6.742779949618587e+17\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Could have also written\n",
    "# x = tf.Variable(2.)\n",
    "# but not\n",
    "# x = tf.Variable(2)\n",
    "# Integer types are not considered differentiable by default.\n",
    "x = tf.Variable(2, dtype=tf.float64)\n",
    "y = tf.Variable(3, dtype=tf.float64)\n",
    "with tf.GradientTape() as tape:\n",
    "  f = x**(2*y**3)\n",
    "\n",
    "df = tape.gradient(f, [x, y])\n",
    "\n",
    "print(f\"F(2,3) = {f.numpy()}\")\n",
    "print(f\"df/dx | (x,y)=(2,3) = {df[0].numpy()}\")\n",
    "print(f\"df/dy | (x,y)=(2,3) = {df[1].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also take second-order derivatives. Note that a GradientTape is released as soon as you call the function `gradient`, unless you make it a persistent object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d2 f / dx2 =  1.288930213353436e+19\n",
      "d2 f / dxdy =  1.8691894623726203e+19\n",
      "d2 f / dydx =  1.86918946237262e+19\n",
      "d2 f / dy2 =  2.5687708783864877e+19\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2, dtype=tf.float64)\n",
    "y = tf.Variable(3, dtype=tf.float64)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as t2:\n",
    "  with tf.GradientTape() as t1:\n",
    "    f = x**(2*y**3)\n",
    "  df = t1.gradient(f, [x, y])\n",
    "d_df_dx = t2.gradient(df[0], [x, y])\n",
    "d_df_dy = t2.gradient(df[1], [x, y])\n",
    "\n",
    "print(\"d2 f / dx2 = \", d_df_dx[0].numpy())\n",
    "print(\"d2 f / dxdy = \", d_df_dx[1].numpy())\n",
    "print(\"d2 f / dydx = \", d_df_dy[0].numpy()) # Should be the same as d2 f / dxdy\n",
    "print(\"d2 f / dy2 = \", d_df_dy[1].numpy())\n",
    "del(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, constants and tensors are not watched. You can instruct the `GradientTape` to watch them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dw = [[  8.  16.  24.  32.]\n",
      " [ -1.  -2.  -3.  -4.]\n",
      " [ -8. -16. -24. -32.]\n",
      " [  6.  12.  18.  24.]]\n",
      "dz/dx = [[ 0.5]\n",
      " [16. ]\n",
      " [ 6. ]\n",
      " [ 8. ]]\n",
      "dz/db = [[ 8.]\n",
      " [-1.]\n",
      " [-8.]\n",
      " [ 6.]]\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(np.array([0, 2, 0, 0,  -0.5, 0, 0, 0,  0, 0, 0, -1,  0, 0, 1, 0]).reshape(4, 4), dtype=tf.float64)\n",
    "x = tf.Variable(np.array([1, 2, 3, 4]).reshape(4,1), dtype=tf.float64)\n",
    "b = tf.constant(np.array([0, 0, 0, 0]).reshape(4, 1), dtype=tf.float64)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  tape.watch(w)\n",
    "  tape.watch(b)\n",
    "  y = w @ x + b\n",
    "  z = tf.reduce_sum(y**2)\n",
    "\n",
    "dz_dw = tape.jacobian(z, w)\n",
    "dz_dx = tape.jacobian(z, x)\n",
    "dz_db = tape.jacobian(z, b)\n",
    "print(f\"dz/dw = {dz_dw.numpy()}\")\n",
    "print(f\"dz/dx = {dz_dx.numpy()}\")\n",
    "print(f\"dz/db = {dz_db.numpy()}\")\n",
    "del(tape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Mean squared error (MSE) function for a linear data with uncertainties\n",
    "\n",
    "Collect 100 random data points between $0 \\le x \\le 1$ that could fit $y=3x+2$, with $\\sigma_y=0.1$.\n",
    "\n",
    "Write an MSE to describe the data and fit it using TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted a = 3.004564747032262, b = 2.0078125173660473\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xdata = np.random.rand(100, 1)\n",
    "ydata = 3*xdata + 2 + np.random.randn(100, 1)/10\n",
    "\n",
    "a = tf.Variable(0, dtype=tf.float64)\n",
    "b = tf.Variable(0, dtype=tf.float64)\n",
    "x = tf.constant(xdata, dtype=tf.float64)\n",
    "yobs = tf.constant(ydata, dtype=tf.float64)\n",
    "\n",
    "learning_rate = 0.05\n",
    "for i in range(2000):\n",
    "  with tf.GradientTape() as tape:\n",
    "    y = a*x+b\n",
    "    mse = tf.reduce_sum((y - yobs)**2/xdata.shape[0])\n",
    "  da, db = tape.gradient(mse, [a, b])\n",
    "  a.assign_sub(learning_rate*da)\n",
    "  b.assign_sub(learning_rate*db)\n",
    "\n",
    "del(tape)\n",
    "\n",
    "plt.scatter(xdata, ydata)\n",
    "plt.plot(xdata, a*xdata+b, 'r')\n",
    "\n",
    "print(f\"Fitted a = {a.numpy()}, b = {b.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
