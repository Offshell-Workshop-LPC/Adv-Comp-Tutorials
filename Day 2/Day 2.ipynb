{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Automatic differentiation, and introduction to CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation\n",
    "\n",
    "Having knowledge of the gradients of a multivariable function with respect to its arguments is a powerful tool in mathematical computations, e.g., in\n",
    "- computing Hessians for optimization problems or extracting uncertainties in a likelihood:\n",
    "$$\\begin{bmatrix} \\frac{\\delta^2 f}{\\delta x_1^2} & \\frac{\\delta^2 f}{\\delta x_1 \\delta x_2} \\\\ \\frac{\\delta^2 f}{\\delta x_1 \\delta x_2} & \\frac{\\delta^2 f}{\\delta x_2^2} \\end{bmatrix}$$\n",
    "- computing gradients to find descent to minimum loss in machine learning:\n",
    "$$\\vec{x}_{n+1} = \\vec{x}_n - \\gamma \\nabla L(\\vec{x})|_{\\vec{x}=\\vec{x}_n}$$\n",
    "\n",
    "\n",
    "There are several ways to compute gradients:\n",
    "- Use symbolic language that maps function names to their definitions and other functions for derivatives. Here is an example using Mathematica:\n",
    "![Derivatives in Mathematica](mathematica_screnshot_diff.png)\n",
    "- Use numerical differentiation with small step sizes.\n",
    "- Embed differentials into variable definitions in programming (**automatic differentiation** - topic of today!).\n",
    "\n",
    "Suppose we have a function $f(g(h(x)))$. The chain rule gives us\n",
    "$$v=h(x)$$\n",
    "$$u=g(v)$$\n",
    "$$y=f(u)$$\n",
    "$$\\frac{\\delta f}{\\delta x} = \\frac{\\delta y}{\\delta u} \\frac{\\delta u}{\\delta v} \\frac{\\delta v}{\\delta x}$$\n",
    "\n",
    "This gives us two directions from which we can start differentiation:\n",
    "- Forward: $\\frac{\\delta y}{\\delta u} \\frac{\\delta u}{\\delta v} \\frac{\\delta v}{\\delta x} \\leftarrow$ start computing derivatives in this direction.\n",
    "- Reverse: start computing derivatives in this direction $\\rightarrow \\frac{\\delta y}{\\delta u} \\frac{\\delta u}{\\delta v} \\frac{\\delta v}{\\delta x}$.\n",
    "\n",
    "Why does it matter? Don't we have the same number of derivatives?\n",
    "\n",
    "$\\rightarrow$ That is correct, but for a function $f: U^n \\to V^m$, we would have a different number of matrix multiplication steps (derivatives $\\equiv$ Jacobian matrices). For the sake of example, assume the dimensionality $k$ for both $g$ and $h$. Then,\n",
    "- forward differentiation involves $(n \\times k^2) + (n \\times k \\times m)$ multiplications, whereas\n",
    "- reverse differentiation would need $(m \\times k^2) + (n \\times k \\times m)$ multiplications.\n",
    "\n",
    "If $n \\cong m$, or $k$ is not large compared to $n$ or $m$, the direction does not matter much. On the other hand, if $k$ is large, it is better to use forward differentiation for $n \\ll m$, and reverse differentiation otherwise.\n",
    "\n",
    "In a typical neural network, gradient descent uses *forward propagation* to calculate the cost for a given loss function , and *backpropagation* (through reverse autodifferentiation - **why?**) to calculate gradients of the cost with respect to the weights at each layer.\n",
    "\n",
    "### Exercise: Dual numbers\n",
    "Consider an extension of a real variable $r \\to (r, \\delta r)$. Denoting $u = (x, \\delta x)$ and $v = (y, \\delta y)$, define the minimal set of algebraic properties as follows:\n",
    "- $-u = (-x, -\\delta x)$\n",
    "- $u^{-1} = (1/x, -\\delta x/x^2)$\n",
    "- $u+v = (x+y, \\delta x + \\delta y)$\n",
    "- $u \\times v = (x \\times y, y \\times \\delta x + x \\times \\delta y)$\n",
    "\n",
    "For $f(x, y) = x^{2 y^3}$, write a simple Python script to compute $f(2,3)$ and $\\frac{\\delta f}{\\delta y} |_{(x,y)=(2,3)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18014398509481984 6.742779949618588e+17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DualValue:\n",
    "  def __init__(self, value, differential):\n",
    "    self.value = value\n",
    "    self.differential = differential\n",
    "  def __add__(self, other):\n",
    "    return DualValue(self.value + other.value, self.differential + other.differential)\n",
    "  def __radd__(self, other):\n",
    "    return DualValue(self.value + other, self.differential)\n",
    "  def __sub__(self, other):\n",
    "    return DualValue(self.value - other.value, self.differential - other.differential)\n",
    "  def __rsub__(self, other):\n",
    "    return DualValue(other - self.value, -self.differential)\n",
    "  def __mul__(self, other):\n",
    "    return DualValue(self.value * other.value, self.differential * other.value + self.value * other.differential)\n",
    "  def __rmul__(self, other):\n",
    "    return DualValue(self.value * other, self.differential * other)\n",
    "  def __truediv__(self, other):\n",
    "    return DualValue(self.value / other.value, (self.differential * other.value - self.value * other.differential) / (other.value * other.value))\n",
    "  def __rtruediv__(self, other):\n",
    "    return DualValue(other / self.value, -self.differential * other / (self.value * self.value))\n",
    "  def __neg__(self):\n",
    "    return DualValue(-self.value, -self.differential)\n",
    "  def __lt__(self, other):\n",
    "    return self.value < other.value\n",
    "  def __le__(self, other):\n",
    "    return self.value <= other.value\n",
    "  def __eq__(self, other):\n",
    "    return self.value == other.value\n",
    "  def __ne__(self, other):\n",
    "    return self.value != other.value\n",
    "  def __gt__(self, other):\n",
    "    return self.value > other.value\n",
    "  def __ge__(self, other):\n",
    "    return self.value >= other.value\n",
    "  def __abs__(self):\n",
    "    return DualValue(abs(self.value), self.differential * np.sign(self.value))\n",
    "  def __str__(self):\n",
    "    return str(self.value)\n",
    "  def __pow__(self, other):\n",
    "    if isinstance(other, DualValue):\n",
    "      return DualValue(self.value**other.value, self.value**other.value * other.differential * np.log(self.value) + self.differential * other.value * self.value**(other.value - 1))\n",
    "    else:\n",
    "      return DualValue(self.value**other, other * self.value**(other - 1) * self.differential)\n",
    "  def __rpow__(self, other):\n",
    "    return DualValue(other**self.value, other**self.value * self.differential * np.log(other))\n",
    "  \n",
    "x = DualValue(2, 0)\n",
    "y = DualValue(3, 1)\n",
    "\n",
    "f = x**(2*y**3)\n",
    "print(f.value, f.differential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
